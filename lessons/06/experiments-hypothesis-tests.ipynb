{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Experiments and Hypothesis Testing\n",
    "\n",
    "_Authors: Alexander Egorenkov (DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Explain the difference between causation and correlation.\n",
    "- Determine causality and sampling bias using directed acyclic graphs.\n",
    "- Identify what missing data is and how to handle it.\n",
    "- Test a hypothesis using a sample case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Data Source](#data-source)\n",
    "\t- [What Are the Features/Covariates/Predictors?](#what-are-the-featurescovariatespredictors)\n",
    "\t- [What Is the Outcome/Response?](#what-is-the-outcomeresponse)\n",
    "\t- [What Do You Think Each Row in the Data Set Represents?](#what-do-you-think-each-row-in-the-dataset-represents)\n",
    "- [Math Review](#math-review)\n",
    "\t- [Covariance](#covariance)\n",
    "\t- [Correlation](#correlation)\n",
    "\t- [The Variance-Covariance Matrix](#the-variance-covariance-matrix)\n",
    "- [Causation and Correlation](#causation-and-correlation)\n",
    "\t- [Structure of Causal Claims](#structure-of-causal-claims)\n",
    "\t- [Why Do We Care?](#why-do-we-care)\n",
    "\t- [How Do We Determine if Something is Causal?](#how-do-we-determine-if-something-is-causal)\n",
    "- [The Pearlean Causal DAG Model](#pearlean-causal-dag-model)\n",
    "\t- [What Is a DAG?](#what-is-a-dag)\n",
    "\t- [X Causes Y](#its-possible-that-x-causes-y)\n",
    "\t- [Y Causes X](#y-causes-x)\n",
    "\t- [The Correlation Between X and Y Is Not Statistically Significant](#the-correlation-between-x-and-y-is-not-statistically-significant)\n",
    "\t- [X or Y May Cause One or the Other Indirectly Through Another Variable](#x-or-y-may-cause-one-or-the-other-indirectly-through-another-variable)\n",
    "\t- [There is a Third Common Factor That Causes Both X and Y](#there-is-a-third-common-factor-that-causes-both-x-and-y)\n",
    "\t- [X and Y Cause a Third Factor, But Our Data Collect the Third Factor Unevenly](#both-x-and-y-cause-a-third-variable-and-the-dataset-does-not-represent-that-third-variable-evenly)\n",
    "\t- [Controlled Experiments](#controlled-experiments)\n",
    "\t- [When Is it OK to Rely on Association?](#when-is-it-ok-to-rely-on-association)\n",
    "\t- [How Does Association Relate to Causation?](#how-does-association-relate-to-causation)\n",
    "- [Sampling Bias](#sampling-bias)\n",
    "\t- [Forms of Sampling Bias](#forms-of-sampling-bias)\n",
    "\t- [Problems From Sampling Bias](#problems-from-sampling-bias)\n",
    "\t- [Recovering From Sampling Bias](#recovering-from-sampling-bias)\n",
    "    - [Stratified Random Sampling](#stratified-random-sampling)\n",
    "- [Missing Data](#missing-data)\n",
    "\t- [Types of Missing Data](#types-of-missing-data)\n",
    "\t- [De Minimis](#de-minimis)\n",
    "\t- [Class Imbalance](#class-imbalance)\n",
    "    - [Relation to Machine Learning](#relation-to-machine-learning)\n",
    "- [Introduction to Hypothesis Testing](#introduction-to-hypothesis-testing)\n",
    "\t- [Validate Your Findings](#validate-your-findings)\n",
    "\t- [Confidence Intervals](#confidence-intervals)\n",
    "\t- [Error Types](#error-types)\n",
    "- [Scenario](#scenario)\n",
    "\t- [Exercises](#exercises)\n",
    "\t- [Statistical Tests](#statistical-tests)\n",
    "\t- [Interpret Your Results](#interpret-your-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"data-source\"></a>\n",
    "## Data Source\n",
    "\n",
    "---\n",
    "\n",
    "Today, we’ll use advertising data from an example in the book [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/).\n",
    "- This is a well-known, standard introduction to machine learning.\n",
    "- The book has a more advanced version — [Elements of Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/) — if you are comfortable with linear algebra and statistics at the graduate level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Code-Along: Bring in Today's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This allows plots to appear directly in the notebook.\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data into a DataFrame.\n",
    "\n",
    "# We use index_col to tell Pandas that the first column in the data has row labels.\n",
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Examine the data with .head(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Questions About the Advertising Data\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this new device. The company might ask you the following: \"On the basis of this data, how should we spend our advertising money in the future?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"what-are-the-featurescovariatespredictors\"></a>\n",
    "### What are the Features/Covariates/Predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-the-outcomeresponse\"></a>\n",
    "### What Is the Outcome/Response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-do-you-think-each-row-in-the-dataset-represents\"></a>\n",
    "### What Do You Think Each Row in the Data Set Represents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"math-review\"></a>\n",
    "## Math Review\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"covariance\"></a>\n",
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Covariance is a measure of the joint variability between two random variables.\n",
    "\n",
    "You can think of this as a measure of linear association. If you have a variance of Y and a variance of X, the covariance is the amount of variance they share.\n",
    "\n",
    "$$cov(X, Y) = \\frac {\\sum{(x_i - \\bar{X})(y_i - \\bar{Y})}} {n}$$\n",
    "\n",
    "> We can gain insight into covariance by looking closely at the formula above. First, observe that the formula effectively pairs the first $x$ data point with the first $y$ data point: $(x_1, y_1)$. All computations are done solely on these pairs of points.\n",
    "\n",
    "> Second, let's ask ourselves, **when would covariance be positive**? From the numerator, covariance would be positive if, for all pairs of data points, $(x_i - \\bar{X})$ and $(y_i - \\bar{Y})$ are 1) both positive or 2) both negative. This occurs when: 1) Both data points are greater than their respective means. Or when: 2) Both data points are less than their respective means! So, if the $x$ data points vary from their mean in the same way the $y$ data points vary from their mean, covariance will be positive.\n",
    "\n",
    "> Third, let's consider: **Might outliers affect covariance?** Yes! Given the structure of the formula (a sum of terms), a large outlier pair far from the means could strongly pull the covariance in one direction.\n",
    "\n",
    "**Covariance Expressed Using Matrix Notation**\n",
    "\n",
    "$$cov(\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}[(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}])]$$\n",
    "\n",
    "**A Useful Special Case (Used Below)**\n",
    "\n",
    "$$cov(X, X) = \\frac {\\sum{(x_i - \\bar{X})^2}} {n} = var(X) = \\sigma_X^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"correlation\"></a>\n",
    "### Correlation\n",
    "\n",
    "While covariance is a useful measure, it can be difficult to compare covariances, as they are not standardized. \n",
    "\n",
    "Instead we can use the correlation, which measures the same effect but reports it as a range from -1 to 1. 1 represents perfect covariance and correlation, 0 represents no correlation, and -1 represents perfect inverse correlation.\n",
    "\n",
    "$$corr(X,Y) = \\frac {cov(X,Y)} {\\sigma_X\\sigma_Y} = \\frac {\\mathbb{E}[(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}])]} {\\sigma_X\\sigma_Y}$$\n",
    "\n",
    "Note that the variance is always positive, making the denominator positive. So, the sign of the covariance between $X$ and $Y$ is the same as the sign of their correlation! \n",
    "\n",
    "The following visual examples better illustrate how correlation refers to how $X$ and $Y$ change together. Notice that a correlation number by itself is not always indicative of the relationship between the variables — always try to supplement 2-D correlation with a visual!\n",
    "\n",
    "![](../assets/images/correlation_examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"the-variance-covariance-matrix\"></a>\n",
    "### The Variance-Covariance Matrix\n",
    "\n",
    "For our purposes in modeling and machine learning, the fastest way to get a preview of the underlying relationships in our data is to use the variance-covariance matrix.\n",
    "\n",
    "The variance-covariance matrix shows the covariance between every variable in our data set.\n",
    "\n",
    "Given $n$ features from $X_1$ to $X_n$, the variance-covariance matrix looks like this (recall that $cov(X, X) = var(X)$):\n",
    "\n",
    "$$\\left[ \\begin{array}{c}\n",
    "var(X_1) & cov(X_1,X_2) & ... & cov(X_1,X_n)  \\\\\n",
    "cov(X_2,X_1) & var(X_2) & ... & cov(X_2,X_n)  \\\\\n",
    "... & ... & ... & ... \\\\\n",
    "cov(X_n,X_1) & cov(X_n,X_2) & ... & var(X_n)\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "From a quick glance at this matrix, we can glean insight about which variables might be strongly correlated. This may also indicate redundant features and/or affect some models.\n",
    "\n",
    "If data are centered around the mean, every column has its mean subtracted from itself. So, the mean for every column is now 0. You can then compute the variance-covariance matrix as:\n",
    "\n",
    "$$\\frac {X^TX} {n}$$\n",
    "\n",
    "Those of you who have been exposed to linear regression may recognize this term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Calculate the variance-covariance matrix. Make sure to first de-mean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the correlation matrix using the DataFrame's built-in `.corr()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we have a large amount of data, the correlation matrix may be too difficult to read. It can help to make a plot.\n",
    "\n",
    "#### Use Seaborn's `.heatmap()` function to make a plot of the correlation matrix.\n",
    "\n",
    "- Remember that we imported Seaborn as `sns`.\n",
    "- To make a correlation matrix from a DataFrame, try `my_df.corr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, looking at linear association doesn't show us the whole picture. We can get a more detailed look with a scatterplot matrix.\n",
    "\n",
    "#### Use Seaborn's `.pairplot()` function to make joint scatterplots of the data.\n",
    "\n",
    "- See if you can guess or [figure out](http://seaborn.pydata.org/generated/seaborn.pairplot.html) how `pairplot()` might work.\n",
    "- `pairplot()` plots each column against each column of a DataFrame. So, at the minimum you must have to pass in the DataFrame you want to analyze!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"causation-and-correlation\"></a>\n",
    "## Causation and Correlation\n",
    "---\n",
    "\n",
    "**Objective**: Explain the difference between causation and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Think of various examples of studies you’ve seen in the media related to food:\n",
    "    - \"[Study links coffee consumption to decreased risk of colorectal cancer](https://news.usc.edu/97761/new-study-links-coffee-consumption-to-decreased-risk-of-colorectal-cancer/)\"\n",
    "    - \"[Coffee does not decrease risk of colorectal cancer](http://news.cancerconnect.com/coffee-does-not-decrease-risk-of-colorectal-cancer/)\"\n",
    "\n",
    "There's a whole book series based on these [Spurious Correlations](http://www.tylervigen.com/spurious-correlations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**So, why are these spurious correlations so common?**\n",
    "\n",
    "- Is it because of sensational headlines?\n",
    "- There's a neglect of robust data analysis.\n",
    "- Causal claims and associations are difficult to convey in an unambiguous way.\n",
    "\n",
    "The food claims above are **correlated** but may or may not be **causal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"structure-of-causal-claims\"></a>\n",
    "### Structure of Causal Claims\n",
    "\n",
    "- If X happens, Y must happen.\n",
    "- If Y happens, X must have happened.\n",
    "  - (You need X and something else for Y to happen.)\n",
    "- If X happens, Y will probably happen.\n",
    "- If Y happens, X probably happened.\n",
    "\n",
    "> **Note:** Properties from definitions are not causal. If some shape is a triangle, it's implied that it has three sides. However, it being a triangle does not _cause_ it to have three sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"why-do-we-care\"></a>\n",
    "### Why Do We Care?\n",
    "\n",
    "- Understanding this difference is critical for executing the data science workflow, especially when identifying and acquiring data.\n",
    "- We need to fully articulate our question and use the right data to answer it while also considering any **confounders**.\n",
    "\n",
    "> **Confounders** are unobserved variables that could affect the outcome. If we neglect to include confounding variables in an analysis, we could easily produce an inaccurate model. For example, we might falsely assume that eating more ice cream cones causes us to wear fewer layers of clothing. In actuality, eating ice cream is correlated with a confounding variable — temperature! To perform an accurate analysis, we can only conclude that ice cream consumption is _correlated with_ clothing layers.\n",
    "\n",
    "- We don’t want to overstate what our model measures.\n",
    "- Be careful not to say “caused” when you really mean “measured” or “associated.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"how-do-we-determine-if-something-is-causal\"></a>\n",
    "### How Do We Determine if Something is Causal?\n",
    "\n",
    "Considering causal criteria is one approach to assessing causal relationships.\n",
    "\n",
    "However, it’s hard to define universal causal criteria.\n",
    "\n",
    "One attempt that's commonly used in the medical field is based on work by Bradford Hill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "He developed a list of “tests” that an analysis must pass in order to indicate a causal relationship:\n",
    "\n",
    "\n",
    "- Strength of association\n",
    "- Consistency\n",
    "- Specificity\n",
    "- Temporality\n",
    "- Biological gradient\n",
    "- Plausibility\n",
    "- Coherence\n",
    "- Experiment\n",
    "- Analogy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Strength (effect size)**: A small association does not mean that there is not a causal effect, although the larger the association, the more likely the effect is to be causal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Consistency (reproducibility)**: Consistent findings observed by different persons in different places with different samples strengthens the likelihood of an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Specificity**: Causation is likely if there is a very specific population at a specific site and a disease with no other likely explanation. The more specific an association between a factor and an effect, the greater the probability of a causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Temporality**: The effect has to occur after the cause (and, if there is an expected delay between the cause and expected effect, then the effect must occur after that delay)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Biological gradient**: Greater exposure should generally lead to greater incidence of the effect. However, in some cases, the mere presence of the factor can trigger the effect. In other cases, an inverse proportion is observed: greater exposure leads to lower incidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Plausibility**: A plausible mechanism between cause and effect is helpful (but Hill noted that knowledge of the mechanism is limited by current knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Coherence**: Coherence between epidemiological and laboratory findings increases the likelihood of an effect. However, Hill noted that \"... lack of such [laboratory] evidence cannot nullify the epidemiological effect on associations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Experiment**: \"Occasionally it is possible to appeal to experimental evidence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Analogy**: The effect of similar factors may be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id=\"pearlean-causal-dag-model\"></a>\n",
    "## The Pearlean Causal Directed Acyclic Graph (DAG) Model\n",
    "\n",
    "---\n",
    "### Some Quick Background Notes:\n",
    "\n",
    "- This model is a visual tool to help us reason about causality and association.\n",
    "- It was proposed by Judea Pearl, although there are many similar models.\n",
    "- We will only scratch the surface, so look into other resources if you're interested in learning more.\n",
    "    - We'll cover the basic idea and most notable cases.\n",
    "    - We won't talk about the formal mathematics or underlying probability, or how to use d-seperation to infer causality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"what-is-a-dag\"></a>\n",
    "### What Is a DAG?\n",
    "- DAG stands for directed acyclic graph; it's a collection of nodes connected by lines. \n",
    "- Each line has an arrow to point in a direction.\n",
    "- If you follow the arrows, you reach a final node. There are no loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A single circle or node in a causal DAG represents an event; something that happens at one point in time.\n",
    "\n",
    "![](./assets/images/dag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's pretend random variables X and Y — or two different types of events — are correlated with each other.\n",
    "\n",
    "**What are the possible causal structures that would produce this correlation?**\n",
    "\n",
    "- X causes Y.\n",
    "- Y causes X.\n",
    "- There is no actual causation.\n",
    "- X or Y indirectly causes the other.\n",
    "- There is a third factor that causes both.\n",
    "- X and Y cause a third factor, but our data collect the third factor unevenly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"its-possible-that-x-causes-y\"></a>\n",
    "### X causes Y.\n",
    "![](./assets/images/x-cause-y.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example where X is a function of Y:\n",
    "X = np.random.randn(100)\n",
    "Y = 5 + 2*X + np.random.randn(100)\n",
    "dag = pd.DataFrame({'X':X, 'Y':Y})\n",
    "\n",
    "# Make a pairplot of the data -- remember that pairplot() takes in a DataFrame!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"y-causes-x\"></a>\n",
    "### Y causes X.\n",
    "![](./assets/images/y-cause-x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example where X is a function of Y:\n",
    "Y = np.random.randn(100)\n",
    "X = 5 + 2*Y + np.random.randn(100)\n",
    "dag = pd.DataFrame({'X':X, 'Y':Y})\n",
    "\n",
    "# Make a pairplot of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"the-correlation-between-x-and-y-is-not-statistically-significant\"></a>\n",
    "### The correlation between X and Y is not statistically significant.\n",
    "![](./assets/images/xy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# No correlation between X and Y:\n",
    "X = np.random.randn(100)\n",
    "Y = 5 + np.random.randn(100)\n",
    "dag = pd.DataFrame({'X':X, 'Y':Y})\n",
    "\n",
    "# Make a pairplot of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"x-or-y-may-cause-one-or-the-other-indirectly-through-another-variable\"></a>\n",
    "### X or Y may cause one or the other indirectly through another variable.\n",
    "![](./assets/images/x-c-z-y.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Y is a function of Z, and Z is a function of X:\n",
    "X = 5 + np.random.randn(100)\n",
    "Z = X + 0.1*np.random.randn(100)\n",
    "Y = 3 + Z + np.random.randn(100)\n",
    "\n",
    "dag = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z})\n",
    "\n",
    "# Make a pairplot of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"there-is-a-third-common-factor-that-causes-both-x-and-y\"></a>\n",
    "### There is a third common factor that causes both X and Y.\n",
    "![](./assets/images/z-cause-xy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Both X and Y are functions of Z:\n",
    "Z = np.random.randn(100)\n",
    "\n",
    "X = 5 + 2*Z + np.random.randn(100)\n",
    "Y = 3 + 3*Z + np.random.randn(100)\n",
    "common_cause = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z})\n",
    "\n",
    "# Make a pairplot of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"both-x-and-y-cause-a-third-variable-and-the-dataset-does-not-represent-that-third-variable-evenly\"></a>\n",
    "### X and Y cause a third factor, but our data collect the third factor unevenly.\n",
    "\n",
    "![](./assets/images/xy-causez.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Z is a function of X and Y:\n",
    "X = 5 + np.random.randn(100)\n",
    "Y = 3 + np.random.randn(100)\n",
    "Z = X + Y + 0.1*np.random.randn(100)\n",
    "common_effect = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z})\n",
    "\n",
    "# Make a pairplot of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally, recovering the causality structure from a correlation matrix is difficult or at times impossible. However, thinking through causal effects can give you a much better intuition regarding your variables and your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Is a \"Confounder\"?\n",
    "\n",
    "Let’s say we performed an analysis to understand what causes lung cancer. \n",
    "\n",
    "We find that people who carry cigarette lighters are 2.4 times more likely to contract lung cancer than people who don’t carry lighters.\n",
    "\n",
    "Does this mean that the lighters are causing cancer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we've noted before, if lighters and cancer are both caused by smoking, there will be a correlation between lighters and cancer. This isn't the only possible diagram, but it makes the most sense.\n",
    "![](./assets/images/smoke-lighter-cancer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we believe the structure above, conditioning on smoking by only looking at non-smokers removes the correlation between lighters and cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"controlled-experiments\"></a>\n",
    "### Controlled Experiments\n",
    "\n",
    "- The most foolproof way to measure an effect is to control all of the confounders and directly intervene and control our variable of interest. \n",
    "- This way we know that any correlation we find is not because of the confounders but instead because of the variable we control. \n",
    "- This also means that all the effects we see are due to the variable we control.\n",
    "- However, experiments are not always possible and take longer than using observational data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"when-is-it-ok-to-rely-on-association\"></a>\n",
    "### When Is it OK to Rely on Association?\n",
    "\n",
    "- **When any intervention that arises from your model affects only the outcome variable.**\n",
    "    - In other words, you only need to predict Y.\n",
    "    - This works because we only need to observe explanatory variables and implicitly know the confounders' effect.\n",
    "    - Decision-making and intervention based on your model are hidden dangers that can shift confounders.\n",
    "    - You can always retrain your model to work with a new set of confounders if they shift.\n",
    "\n",
    "- **When correlation is causal.**\n",
    "    - If you are sure there are no confounding factors or selection bias, then that association might be a causation (risky).\n",
    "    - It's OK to exclude confounders that have very unlikely or small effects.\n",
    "    - This is a saving grace. To create a good model, you only need variables that correlate with your outcome.\n",
    "        - Those variables merely need to meaningfully correlate with your outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"how-does-association-relate-to-causation\"></a>\n",
    "### How Does Association Relate to Causation?\n",
    "\n",
    "- Most commonly, we find an association between two variables.\n",
    "    - There is an observed correlation between the variables.\n",
    "    - There is an observed correlation in a subset of data.\n",
    "    - We find that the descriptive statistics significantly differ in two subsets of data.\n",
    "\n",
    "- We may not still fully understand the causal direction (e.g., does smoking cause cancer or does cancer cause smoking?).\n",
    "    - A causes B, B causes A, or a third factor causes both.\n",
    "        - A and B never cause each other!\n",
    "\n",
    "- We also might not understand other factors influencing the association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Confounding variables often hide the true association between causes and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A directed acyclic graph (DAG) can help determine which variables are most important for your model. It helps to visually demonstrate the logic of your models.\n",
    "\n",
    "A DAG always includes at least one exposure/predictor and one outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code-Along: Explore the Associations in the Advertising Data\n",
    "\n",
    "#### Visualize the relationship between the features and the response using scatterplots.\n",
    "\n",
    "- Below, we filled in how to make a scatterplot for the columns `sales` vs `TV`. \n",
    "- Using this as an example, can you also make scatterplots for `sales` vs `radio` and `sales` vs `newspaper`?\n",
    "- `axs[0]` is the first coordinate grid, `axs[1]` is the second coordinate grid, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between the features and the response using scatterplots:\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "\n",
    "data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a relationship between ads and sales? Which type of ads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we say this a causal relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What other questions might we want to know about this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Group Exercise: Evaluate Which Type of Ad is Associated With Higher Sales\n",
    "\n",
    "Let's say we want to evaluate which type of ad is associated with higher sales.\n",
    "\n",
    "1. Draw a basic DAG on your table or whiteboard.\n",
    "    - Think about other variables that may predict sales.\n",
    "    - Think about confounders.\n",
    "    - Think about the downstream effects changing investment in advertising.\n",
    "    - Be ready to share an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Section Summary\n",
    "\n",
    "1) **It's importlant to have deep subject area knowledge.** You'll develop this over time and it will help you move through your analysis in a logical manner. However, keep in mind that you can show a strong association and still be wrong.\n",
    "\n",
    "~~2) **A DAG (directed acyclic graph) can be a handy tool for thinking through the logic of your models.**~~\n",
    "\n",
    "3) **There is a distinction between causation and correlation.** In our smoking example, it's relatively obvious that there's a flaw in our logic; however, this won't always be so readily apparent — especially in cutting-edge fields where there are many other unknown variables.\n",
    "\n",
    "4) **Good data are essential.** Throughout this course we will be developing your data intuition so you can spot gaps and bias more readily. You'll also be introduced to tools that can help. However, your analysis is only as good as your understanding of the problem and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"sampling-bias\"></a>\n",
    "## Sampling Bias\n",
    "---\n",
    "\n",
    "**Sampling bias** occurs when a sample is collected in such a way that some members of the intended population are more or less likely to be included than others.\n",
    "\n",
    "This can happen when a sample is taken non-randomly — either implicitly or explicitly.\n",
    "\n",
    "When we have non-random sampling that results in sampling bias, it can affect the inferences or results of our analyses. We must be sure not to attribute our results to the process we observe when they could actually be because of non-random sampling.\n",
    "\n",
    "Conceptually, this is straightforward: When we have sampling bias, we aren't measuring what we think we are measuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"forms-of-sampling-bias\"></a>\n",
    "### Examples of Sampling Bias\n",
    "\n",
    "- **Pre-screening:** Purposely restricting the sample to a specific group or region.\n",
    "    - This typically happens when people try to study priority areas to save costs and assume priority areas are the same as random areas.\n",
    "- **Self-selection:** When someone has the ability to non-randomly decide what is included in a sample.\n",
    "    - This typically happens in surveys and polls but can also be an issue with other kinds of reporting.\n",
    "- **Survivorship bias:** When we select only surviving subjects in a sample over time.\n",
    "    - This might happen when we only look at existing customers and assume they have the same characteristics as new customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"problems-from-sampling-bias\"></a>\n",
    "### Problems That Arise From Sampling Bias\n",
    "- We could overestimate or underestimate means and sample statistics for simple characteristics.\n",
    "- It's possible to have artificial correlation where there should be none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"recovering-from-sampling-bias\"></a>\n",
    "### Recovering From Sampling Bias\n",
    "~~- Working out causal DAGs can help you identify when to watch out for sampling bias.~~\n",
    "- Generally, it's best to prevent sampling bias whenever possible.\n",
    "- We can't really do anything if we ENTIRELY exclude an important group of data.\n",
    "- However, if portions of our data are overrepresented or underrepresented, there are ways to correct that effect.\n",
    "    - Typically, we explicitly model the selection process, which means we need data on factors that determine whether  or not someone participates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"stratified-random-sampling\"></a>\n",
    "### Stratified Random Sampling\n",
    "\n",
    "We've discussed how it is important to obtain a random sample of our population. However, sometimes it is more effective to apply some reasoning to our sampling process. By optimizing how we choose samples, we can possibly create a more accurate model using fewer samples.\n",
    "\n",
    "- **Stratified random sampling** ensures we capture important population characteristics in the random sample. If we know that the population is half males and half females, for example, we can make sure that our sample is half male and half female. We effectively break the population into two \"strata\" (groups), then randomly sample from each group to obtain our overall sample. This method is similar to taking a weighted average and depends on knowing key population statistics.\n",
    "    - For example, if we are collecting survey data, we might ensure our participants are evenly split between men and women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"missing-data\"></a>\n",
    "## Missing Data\n",
    "---\n",
    "\n",
    "Sometimes we are unable to collect every attribute for a particular observation.\n",
    "\n",
    "Unfortunately, this makes the observation unusable until we decide how to deal with it.\n",
    "\n",
    "**We have to decide whether to:**\n",
    "    - Drop the observation.\n",
    "    - Drop the attribute.\n",
    "    - Impute a value for that specific attribute and observation.\n",
    "\n",
    "**So, how do we decide?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"types-of-missing-data\"></a>\n",
    "### Types of Missing Data\n",
    "\n",
    "- **Missing completely at random (MCAR)**\n",
    "    - The reason that the data are missing is completely random and introduces no sampling bias.\n",
    "    - In this case, it's safe to drop or impute.\n",
    "    - We can test for this by looking at other attributes for missing and non-missing groups to see if they match.\n",
    "\n",
    "- **Missing at random (MAR)**\n",
    "    - The data are missing in a way that is related to another factor.\n",
    "    - This is a form of sampling bias.\n",
    "    - Like other instances of sampling bias, we can fix this by modeling the selection process.\n",
    "        - This is done by building a model to impute the missing value based on other variables.\n",
    "\n",
    "- **Missing not at random (MNAR)**\n",
    "    - The response is missing in a way that relates to its own value.\n",
    "    - We can't test for this.\n",
    "    - We also can't fix this in a reasonable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"de-minimis\"></a>\n",
    "### De Minimis\n",
    "- If few enough observations are missing, it's not likely to change our results to a meaningful degree.\n",
    "- In these cases, we don't have to bother with trivialities and simply pick a method that works well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"class-imbalance\"></a>\n",
    "### Class Imbalance\n",
    "\n",
    "Sometimes a sample may include an overrepresentation of one type of class. For example, airport security may have 990 X-ray scans showing the absence of a weapon. Due to natural scarcity, it may only provide 10 scans showing a weapon.\n",
    "\n",
    "- If our goal is to create a model that indicates whether or not a weapon is present, then we are at a disadvantage. **Ignoring the class imbalance** would lead to a model that always guesses that a weapon is not present!\n",
    "    - Note that most optimization procedures optimize for training data accuracy. Always guessing that a weapon is absent leads to 990/1000 correct results; an accuracy of 99 percent.\n",
    "\n",
    "- A simple way to get around this is to **undersample** the majority class, deliberately leaving us with a balanced data set of 10 each. However, this is less than ideal, as it effectively ignores much of the available data.\n",
    "\n",
    "- Alternatively, we could **oversample** the minority class by duplicating examples. Again, this is not ideal. Because we have very little data, this will magnify small differences that may just be errors, leading to a model that overfits.\n",
    "\n",
    "Later in the course, we will look at additional methods for training models to work around class imbalance. For example, we may use an optimization algorithm that cares less about accuracy and more about minimizing particular types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"relation-to-machine-learning\"></a>\n",
    "### Relation to Machine Learning\n",
    "\n",
    "Many of the topics discussed in this lesson are used in both statistics and machine learning. However, some of the terminology differs. \n",
    "\n",
    "Throughout this lesson, we have discussed **variables** (typically **independent variables** and **dependent variables**). For example, we might be given the **linear estimator** $Y = mX + b$. We could say that this contains two variables ($X$ - independent and $Y$ - dependent (i.e., the prediction, as it depends on $X$)), a coefficient of $m$, and the constant of $b$.\n",
    "\n",
    "In machine learning, we typically rewrite this as a function — $\\hat{y}(x) = mx + b$ — and call it a **linear model**. The predicted value is $\\hat{y}(x)$ (\"prediction\" is denoted by the carat), which is dependent on $x$. We might call $x$ a **feature** rather than a variable.\n",
    "\n",
    "> **Example:** Suppose a house price $P$ is linearly dependent on its square footage $S$. So, we might predict $P = cS + b$, where $c$ and $b$ are constants. Alternatively, we could write $\\hat{p}(s) = cs + b$. Here, we took a complicated house and modeled it using a single feature — its square footage. Of course, we are likely missing many confounding variables/features that also affect the price! So, our model likely contains a lot of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"introduction-to-hypothesis-testing\"></a>\n",
    "## Introduction to Hypothesis Testing\n",
    "---\n",
    "\n",
    "**Objective**: Test a hypothesis within a sample case study.\n",
    "\n",
    "You'll remember that we've worked previously on descriptive statistics such as mean and variance. How would we tell if there is a difference between our groups? How would we know if this difference was real or if our finding is simply the result of chance?\n",
    "\n",
    "For example, if we are working on sales data, how would we know if there was a difference between the buying patterns of men and women at Acme, Inc.? Hypothesis testing!\n",
    "\n",
    "> **Note:** In this course, hypothesis testing is primarily used to assess foundational models such as linear and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hypothesis Testing Steps\n",
    "\n",
    "Generally speaking, we start with a **null hypothesis** and an **alternative hypothesis**, which is the opposite of the null. Then, you check whether the data support rejecting your null hypothesis or fail to reject the null hypothesis.\n",
    "\n",
    "For example:\n",
    "\n",
    "    Null hypothesis: There is no relationship between gender and sales.\n",
    "    Alternative hypothesis: There is a relationship between gender and sales.\n",
    "\n",
    "Note that \"failing to reject\" the null hypothesis is not the same as \"accepting\" it. Your alternative hypothesis may indeed be true, but you don't necessarily have enough data to show that yet.\n",
    "\n",
    "This distinction is important for helping you avoid overstating your findings. You should only state what your data and analysis can truly represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"validate-your-findings\"></a>\n",
    "### Validate Your Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How Do We Tell if the Association We Observed is Statistically Significant?\n",
    "\n",
    "Statistical significance is the likelihood that a result or relationship is caused by something other than mere random chance. Statistical hypothesis testing is traditionally employed to determine whether or not a result is statistically significant.\n",
    "\n",
    "We might ask: **How likely is the effect observed to be true, assuming the null hypothesis is true?** If the probability of our observation occurring by chance is less than 5 percent (supposing the null hypothesis), then we reject the null hypothesis. Note that the 5 percent value is in many ways arbitrary — many statisticians require even higher confidence levels.\n",
    "\n",
    "The probability of our observations occurring by chance, given the null hypothesis, is the **pvalue** ($p$).\n",
    "\n",
    "---\n",
    "\n",
    "**Example:** Suppose you flip a coin three times and get three heads in a row. These three flips are our observations.\n",
    "\n",
    "+ We want to know whether or not the coin is fair. So, we select the **null hypothesis: The coin is fair.**\n",
    "+ Now, let's suppose the null hypothesis is true. Three heads in a row occurs with a chance of $1/2^3 \\approx 12.5\\%$.\n",
    "+ Because there is a reasonable ($> 5\\%$) chance of three heads occuring naturally, we do not reject the null hypothesis.\n",
    "+ So, **we conclude** that we do not have enough data to tell whether or not the coin is fair ($p = 0.125$).\n",
    "\n",
    "---\n",
    "\n",
    "In other words, we say that something is NOT statistically significant if there is a less than 5 percent chance that our finding was caused by chance alone (assuming the null hypothesis is true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"confidence-intervals\"></a>\n",
    "### Confidence Intervals\n",
    "\n",
    "A closely related concept is **confidence intervals**. A 95 percent confidence interval can be interpreted like so: under infinite sampling of the population, we would expect that the true value of the parameter we are estimating to fall within that range 95% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keep in mind that we only have a **single sample of data** and not the **entire population of the data**. The \"true\" effect/difference is either within this interval or it is not. We have no firm knowledge, however, that our single estimate of the \"true\" effect/difference is close or not to the \"truth\". The confidence interval around our estimate tells us, with a given sample size and level of confidence, the range in which future estimates are likely to fall.\n",
    "\n",
    "Note that using 95 percent confidence intervals is just a convention. You can create 90 percent confidence intervals (which will be more liberal), 99 percent confidence intervals (which will be more conservative), or whatever intervals you prefer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"error-types\"></a>\n",
    "### Error Types\n",
    "\n",
    "Statisticians often classify errors not just as errors but as one of two specific types of errors — type I and type II.\n",
    "\n",
    "+ **Type I errors** are false positives.\n",
    "    - Machine learning: Our model falsely predicts \"positive.\" (The prediction is incorrect.)\n",
    "    - Statistics: Incorrect rejection of a true null hypothesis.\n",
    "\n",
    "\n",
    "+ **Type II errors** are false negatives.\n",
    "    - Machine learning: Our model falsely predicts \"negative.\" (The prediction is incorrect.)\n",
    "    - Statistics: Incorrectly retaining a false null hypothesis.\n",
    "\n",
    "\n",
    "Understanding these errors can be especially beneficial when designing models. For example, we might decide that type I errors are OK but type II errors are not. We can then optimize our model appropriately.\n",
    "\n",
    "> **Example:** Suppose we make a model for airline security in which we predict whether or not a weapon is present (\"positive\"). In this case, we would much rather have type I errors (falsely predict a weapon) than type II errors (falsely predict no weapon).\n",
    "\n",
    "> **Example:** Suppose we make a model for the criminal justice system in which we whether or not a defendant is guilty (\"positive\"). In this case, we would much rather have type II errors (falsely predict innocent) than type I errors (falsely predict guilty).\n",
    "\n",
    "Can you phrase these examples in terms of null hypotheses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Challenge: A/B Testing Hypothesis Tests\n",
    "\n",
    "<a id=\"scenario\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario\n",
    "\n",
    "You are a data science team working for a web-based company and you are planning to roll out a new website design. One of two competing designs were presented to random samples of users, and their ultimate purchase total was recorded (if any).\n",
    "\n",
    "Your task is to determine which of the two designs yields higher total purchases and if the result is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data and randomize.\n",
    "\n",
    "# Some people bought nothing, the others bought \n",
    "# with some distribution.\n",
    "data1 = [0] * 50\n",
    "data1.extend(np.random.normal(14, 4, 150))\n",
    "np.random.shuffle(data1)\n",
    "\n",
    "# The second design hooked less people, \n",
    "# but those that were hooked bought more stuff.\n",
    "data2 = [0] * 100\n",
    "data2.extend(np.random.normal(20, 5, 100))\n",
    "np.random.shuffle(data2)\n",
    "\n",
    "# Make a DataFrame.\n",
    "df = pd.DataFrame()\n",
    "df[\"A\"] = data1\n",
    "df[\"B\"] = data2\n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot out the distributions of group A and group B.\n",
    "\n",
    "- Plot a histogram of ONLY the group A column, and ONLY the group B column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a box plot of the two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are our data sets (approximately) normal? Use what we learned in the previous lesson to decide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distributions for group A and B. Are they approximately normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"statistical-tests\"></a>\n",
    "### Statistical Tests\n",
    "\n",
    "There are a few good statistical tests for A/B testing:\n",
    "* [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance)\n",
    "* [Welch's t-test](https://en.wikipedia.org/wiki/Welch's_t-test)\n",
    "* [Mann-Whitney test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)\n",
    "\n",
    "**Each test makes various assumptions:**\n",
    "* ANOVA assumes the residuals are normally distributed and data have equal variances.\n",
    "* The Welch t-test assumes normal distributions but not necessarily equal variances and more effectively accounts for small sample sizes.\n",
    "* The Mann-Whitney test assumes nothing about the distributions but requires at least 20 data points in each set, producing a weaker p value.\n",
    "\n",
    "Typically you need to choose the most appropriate test. Tests that make more assumptions are more discriminating (producing stronger p values) but can be misleading with data sets that don't satisfy the assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which test is most appropriate for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, **one-way analysis of variance** (abbreviated one-way **ANOVA**) is a technique used to compare the means of three or more samples (using the **F distribution**). The **ANOVA** tests the **null hypothesis** (the default position that there is no relationship) that samples in two or more groups are drawn from populations with the same mean values. Typically, however, the **one-way ANOVA** is used to test for differences among at least three groups, as the two-group case can be covered by a **t-test**. When there are only two means to compare, the **t-test** and the **F-test** are equivalent.\n",
    "\n",
    "> **Note:** \n",
    "   - One-way ANOVA: An ANOVA hypothesis tests the difference in population means based on one characteristic or factor.\n",
    "   - Two-way ANOVA: An ANOVA hypothesis tests comparisons between populations based on multiple characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Mann-Whitney test on our data.\n",
    "\n",
    "- Look up the function in SciPy [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html).\n",
    "- Statistic: Float the Mann-Whitney U statistic — equal to min(U for x, U for y) if alternative is equal to none (deprecated; exists for backward compatibility) — and U for Y otherwise.\n",
    "- P value: Float p value assuming an asymptotic normal distribution — one sided or two sided, depending on the choice of alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon–Mann–Whitney test) is a nonparametric test of the null hypothesis of whether it is equally likely that a randomly selected value from one sample will be less than or greater than a randomly selected value from a second sample.\n",
    "\n",
    "Unlike the t-test, it does not require the assumption of normal distributions. It is also nearly as efficient as the t-test on normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interpret-your-results\"></a>\n",
    "### Interpret Your Results\n",
    "* Is there a significant difference in the mean total purchases in the two designs?\n",
    "* Which design do you recommend? Why? \n",
    "* Write two sentences explaining your results and your recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
